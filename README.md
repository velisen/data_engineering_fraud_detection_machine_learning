<img src="DE-ML.png">
# Â Data Engineering & Fraud Detection with Machine Learning

This project presents a modular, end-to-end pipeline for detecting fraudulent transactions using machine learning. It integrates data ingestion, preprocessing, model training, and inference within a scalable, containerized architecture.

## ğŸš€ Project Overview

The pipeline encompasses the following components:

* **Data Ingestion**: Simulates real-time transaction data using a producer module.
* **Data Orchestration**: Utilizes Apache Airflow for scheduling and managing data workflows.
* **Model Training & Tracking**: Employs MLflow to train models and track experiments.
* **Model Inference**: Applies trained models to incoming data for fraud prediction.
* **Containerization**: Leverages Docker and Docker Compose for environment setup and service orchestration.

## ğŸ§± Project Structure

```
â”œâ”€â”€ airflow/             # Airflow configurations and DAGs
â”œâ”€â”€ dags/                # Airflow DAG definitions
â”œâ”€â”€ inference/           # Scripts for model inference
â”œâ”€â”€ logs/                # Log files
â”œâ”€â”€ mlflow/              # MLflow tracking server setup
â”œâ”€â”€ models/              # Saved machine learning models
â”œâ”€â”€ producer/            # Data producer scripts
â”œâ”€â”€ .env                 # Environment variables
â”œâ”€â”€ config.yaml          # Configuration file
â”œâ”€â”€ docker-compose.yml   # Docker Compose configuration
â”œâ”€â”€ init-multiple-dbs.sh # Script to initialize multiple databases
â””â”€â”€ wait-for-it.sh       # Script to wait for services to be ready
```

## ğŸ› ï¸ Getting Started

### Prerequisites

Ensure you have the following installed:

* [Docker](https://www.docker.com/)
* [Docker Compose](https://docs.docker.com/compose/)

### Installation

1. **Clone the repository**:

```bash
git clone https://github.com/velisen/data_engineering_fraud_detection_machine_learning.git
cd data_engineering_fraud_detection_machine_learning
```

2. **Set up environment variables**:

Create a `.env` file in the root directory and define necessary environment variables as per your configuration.

3. **Build and start the services**:

```bash
docker-compose up --build
```

This command will build the Docker images and start all services, including Airflow, MLflow, and the data producer.

## ğŸ“ˆ Usage

* **Access Airflow**: Navigate to `http://localhost:8080` to access the Airflow web interface.
* **Access MLflow**: Navigate to `http://localhost:5000` to access the MLflow tracking server.
* **Monitor Logs**: Logs are stored in the `logs/` directory for debugging and monitoring purposes.

## ğŸ“„ Configuration

The `config.yaml` file contains configuration settings for the project, including paths, model parameters, and other settings. Modify this file to suit your specific requirements.


